# -*- coding: utf-8 -*-
"""
Created on Thu Sep 07 11:58:02 2017

@author: dnathani
"""

# -*- coding: utf-8 -*-
"""
Created on Wed Sep 06 14:39:48 2017

@author: dnathani
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Aug 28 14:49:47 2017

@author: dnathani
"""

import flask
from flask import request
import os
from flask import Flask
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_recall_fscore_support
import pandas as pd
import pickle
import json
import csv
import random


###############################################################################
#Configuration
###############################################################################


class Configuration():
    def proxies(self):
        proxy = 'http://proxy-src.research.ge.com:8080'
        os.environ['RSYNC_PROXY'] = "proxy-src.research.ge.com:8080"
        os.environ['http_proxy'] = proxy
        os.environ['HTTP_PROXY'] = proxy
        os.environ['https_proxy'] = proxy
        os.environ['HTTPS_PROXY'] = proxy
        os.environ['no_proxy'] = ".ge.com"

###############################################################################
#Flask Configuration
###############################################################################
        
        
app = Flask(__name__, static_url_path="", static_folder="static")


###############################################################################
#Global Variables
###############################################################################
columns=[]
rowsData=[]
FileData=[]
FileResult=[]
attributeValidity=[]
rowNumber=0
FileName=""
Training_data=[]
Testing_data=[]
Training_Result=[]
Testing_Result=[]
Filtered_traning_data=[]
finalAttributes=[]
FinalAttributeCount=0
Filtered_testing_data=[]
KNNSelected=False
DTSelected=False
DT_accuracy=0
KNN_accuracy=0
data_Faltu1=[]
data_Faltu2=[]
AttributeDataJson={}
uniqueData=set()
###############################################################################
#Training
###############################################################################
def reset_Data():
    global columns
    global rowsData
    global FileData
    global FileResult
    global attributeValidity
    global rowNumber
    global FileName
    global Training_data
    global Testing_data
    global Training_Result
    global Testing_Result
    global Filtered_traning_data
    global finalAttributes
    global FinalAttributeCount
    global Filtered_testing_data
    global KNNSelected
    global DTSelected
    global DT_accuracy
    global KNN_accuracy
    global data_Faltu1
    global data_Faltu2
    global AttributeDataJson
    global uniqueData
    
    columns=[]
    rowsData=[]
    FileData=[]
    FileResult=[]
    attributeValidity=[]
    rowNumber=0
    FileName=""
    Training_data=[]
    Testing_data=[]
    Training_Result=[]
    Testing_Result=[]
    Filtered_traning_data=[]
    finalAttributes=[]
    FinalAttributeCount=0
    Filtered_testing_data=[]
    KNNSelected=False
    DTSelected=False
    DT_accuracy=0
    KNN_accuracy=0
    data_Faltu1=[]
    data_Faltu2=[]
    AttributeDataJson={}
    uniqueData=set()

#******************************************************************************
#Attribute Selection (First Call)
#******************************************************************************

@app.route('/dataset/split')
def attribute_selection():
    filename = request.args.get('file')
    Training = int(request.args.get('Training'))
    Testing = int(request.args.get('Testing'))
    Hold = int(request.args.get('Hold'))
    reset_Data()
    global FileName
    FileName = filename
    # TODO:ADD FilePath
    filepath="TODO"
    filepath=os.path.join(r"..\Uploads",FileName)
    with open(filepath,"r") as f:
            global columns
            reader = csv.reader(f,delimiter = ",")
            columns=reader.next()
            global rowsData
            global FileData
            global FileResult
            global rowNumber
            for row in reader:
                rowsData.append(row)
                rowNumber += 1
            for newLine in rowsData:
                temp2=[]
                for dataindex in range(0,(len(columns)-1)):
                    
                    temp2.append(float(newLine[dataindex]))
                FileResult.append(int(newLine[len(columns)-1]))
                FileData.append(temp2)
                
    model = LogisticRegression()
    rfe = RFE(model, int(len(columns)*0.4))
    rfe = rfe.fit(FileData, FileResult)
    global attributeValidity
    attributeValidity=rfe.support_
    return split_Data(Training,Testing,Hold)
    #return "STEP 1 Done"

def split_Data(Training,Testing,Hold):
    # TODO:ADD FilePath
    #filepath=os.path.join(r"C:\Users\dnathani\Desktop\FFT_Trials\ML Trials","creditcard.csv")
    print(Training,Testing,Hold)
    filepath=os.path.join(r"..\Uploads",FileName)
    with open(filepath,"r") as f:
            reader = csv.reader(f,delimiter = ",")
            columns=reader.next()
            
            data = list(reader)
            row_count = len(data)
            trainEnd=int(((row_count*Training)+1)/100)
            
            testEnd=int(int((row_count*Training)+1+int(row_count*Testing)+1)/100)
            
            #randomList=np.random.randint(1,row_count,size=row_count)
            randomList=random.sample(range(1,row_count), row_count-1)
            trainingData=[]
            testingData=[]
            for data in range(0,trainEnd):
                trainingData.append(randomList[data])
            for data in range(trainEnd+1,testEnd):
                testingData.append(randomList[data])
           
      
    with open(filepath,"r") as f:
            rowsData=[]
            reader = csv.reader(f,delimiter = ",")
            for row in reader:
                rowsData.append(row)
    
            global Training_data
            global Testing_data
            global Training_Result
            global Testing_Result           

            
            for data in trainingData:
                Training_data.append(FileData[data])
                Training_Result.append(FileResult[data])
                
            
            for data in testingData:
                Testing_data.append(FileData[data])
                Testing_Result.append(FileResult[data])
            
            
            df = pd.read_csv(filepath)
            
            columnResults=[]
            for data in columns:
                columnResults.append(df[data].values)
            
            new_col_result=[]
            for data in columnResults:
                temp=[]
                for data_inside in data:
                    temp.append(float(data_inside))
                new_col_result.append(temp)
            newAttributeValidity=attributeValidity.tolist()
            groupingData=range(0,len(columns)-1)
            groupingData.append(0)
            global uniqueData
            for data in new_col_result[len(columns)-1]:
                uniqueData.add(data)
            UniqueList=list(uniqueData)
            
            group=[]
            for data in range(0,len(columns)-1):
                group.append(zip(new_col_result[groupingData[data]],new_col_result[groupingData[data+1]]))
            colorData=[]
            for data in range(0,len(columns)):
                tempColor=[]
                tempColor.append(int(random.random()*255))
                tempColor.append(int(random.random()*255))
                tempColor.append(int(random.random()*255))
                colorData.append(tempColor)    
                
            results=[]
            for columnIndex in range(0,len(columns)-1):
                tempJsonData=[]
                colorCount=0
                for classificationData in UniqueList:
                    mesData=[]
                    for data in range(0,len(new_col_result[len(columns)-1])):
                        if new_col_result[len(columns)-1][data] == classificationData:
                            mesData.append(group[columnIndex][data])
                    tempColorData="rgba({}, {}, {}, .5)".format(colorData[colorCount][0],colorData[colorCount][1],colorData[colorCount][2])        
                    tempJsonData.append({"name":classificationData,"data":mesData,"color":tempColorData})
                    colorCount += 1
                if newAttributeValidity[columnIndex]:
                    tempRelevance="Relevent"
                else:
                    tempRelevance="Irrelevent"
                results.append({"attributeName":columns[columnIndex],"attributeData":tempJsonData, "RelevanceValue": tempRelevance,"Relevance":newAttributeValidity[columnIndex]})
                                
            global AttributeDataJson
            
            AttributeDataJson = {"data": results}
    return "Success"


@app.route('/dataset/getAttributes')
def getAttribute():
    global AttributeDataJson
    return json.dumps(AttributeDataJson)
#******************************************************************************
#Filter Data According to Selected Attributes
#******************************************************************************

@app.route('/dataset/sendAttributes', methods=['POST'])    
def filter_data():
    input_json = request.get_json(force=True)  
    Jsonkeys=input_json.keys()
    global finalAttributes
    global FinalAttributeCount
    for data in columns:
        if data in Jsonkeys:
            if input_json[data]:
                finalAttributes.append(data)
                FinalAttributeCount += 1
    
    global Filtered_traning_data
    global Filtered_testing_data
    
    for data in Training_data:
        temp=[]
        for columnData in finalAttributes:
            temp.append(data[columns.index(columnData)])
        Filtered_traning_data.append(temp) 
     
    for data2 in Testing_data:
        temp2=[]
        for columnData in finalAttributes:
            temp2.append(data2[columns.index(columnData)])
        Filtered_testing_data.append(temp2)  
        
        
    return "Step 3 Done"

#******************************************************************************
#Train Decision Tree
#******************************************************************************

@app.route('/dataset/trainModel')    
def train_dicision_tree():
    classifierName = request.args.get('classifier')
    if classifierName.lower() == "decision_tree":
        global DTSelected
        DTSelected=True
        trainerDT=DecisionTreeClassifier()
        global Filtered_traning_data
        trainerDT=trainerDT.fit(Filtered_traning_data,Training_Result)
        #pickle.dump(trainerDT, open(os.path.splitext(FileName)[0]+"_DT.sav", 'wb'))
        #loaded_model_DT = pickle.load(open(os.path.splitext(FileName)[0]+"_DT.sav", 'rb'))
        pickle.dump(trainerDT, open(r".\Models"+os.path.splitext(FileName)[0]+".sav", 'wb'))
        loaded_model_DT = pickle.load(open(r".\Models"+os.path.splitext(FileName)[0]+".sav", 'rb'))
        prediction_DT=loaded_model_DT.predict(Filtered_testing_data)
        global DT_accuracy
        global uniqueData
        DT_accuracy=0
        DT_accuracy += accuracy_score(Testing_Result,prediction_DT)
        precisionDataDT=precision_recall_fscore_support(Testing_Result, prediction_DT)[0].tolist()
        precisionDT=[]
        print(precisionDataDT)
        for data in precisionDataDT:
            precisionDT.append(data/(sum(precisionDataDT)))
        sampleRowData=[]
        for data in Filtered_traning_data[len(Filtered_traning_data)-1]:
            sampleRowData.append(data)
        sampleRowData.append(Training_Result[len(Training_Result)-1])
        finalAttributesList=[]
        finalAttributes.append("Result")
        finalAttributesList=list(set(finalAttributes))
        training_result={"accuracy":DT_accuracy*100,"totalNumberOfRows":rowNumber,"numberOfRowsForTraining":len(Training_data),"numberOfRowsForTesting":len(Testing_data),"totalNumberOfAttributes":len(columns)-1,"numberOfAttributesUsed":FinalAttributeCount,"rowData":sampleRowData,"columnData":finalAttributesList,"classifier":"Decision_Tree","classification":list(uniqueData),"Precision":precisionDataDT}
        return json.dumps(training_result)
    if classifierName.lower() == "knn_classifier":
        global KNNSelected
        KNNSelected=True    
        trainerKNN=KNeighborsClassifier()
        global Filtered_traning_data
        trainerKNN=trainerKNN.fit(Filtered_traning_data,Training_Result)
        #pickle.dump(trainerKNN, open(os.path.splitext(FileName)[0]+"_KNN.sav", 'wb'))
        #loaded_model_KNN = pickle.load(open(os.path.splitext(FileName)[0]+"_KNN.sav", 'rb'))
        pickle.dump(trainerKNN, open(r".\Models"+os.path.splitext(FileName)[0]+".sav", 'wb'))
        loaded_model_KNN = pickle.load(open(r".\Models"+os.path.splitext(FileName)[0]+".sav", 'rb'))
        prediction_KNN=loaded_model_KNN.predict(Filtered_testing_data)
        global KNN_accuracy
        global uniqueData
        KNN_accuracy=0
        KNN_accuracy=accuracy_score(Testing_Result,prediction_KNN)
        precisionDataKNN=precision_recall_fscore_support(Testing_Result, prediction_KNN)[0].tolist()
        precisionKNN=[]
        for data in precisionDataKNN:
            precisionKNN.append(data/(sum(precisionDataKNN)))
        sampleRowData=[]
        for data in Filtered_traning_data[len(Filtered_traning_data)-1]:
            sampleRowData.append(data)
        sampleRowData.append(Training_Result[len(Training_Result)-1])
        finalAttributesList=[]
        finalAttributes.append("Result")
        finalAttributesList=list(set(finalAttributes))
        training_result={"accuracy":KNN_accuracy*100,"totalNumberOfRows":rowNumber,"numberOfRowsForTraining":len(Training_data),"numberOfRowsForTesting":len(Testing_data),"totalNumberOfAttributes":len(columns)-1,"numberOfAttributesUsed":FinalAttributeCount,"rowData":sampleRowData,"columnData":finalAttributesList,"classifier":"KNN_Classifier","classification":list(uniqueData),"Precision":precisionDataKNN}
        return json.dumps(training_result)

#******************************************************************************
#Rename Saved Model
#******************************************************************************


@app.route('/save')    
def rename_savedfile():
    aliceName = request.args.get('modelName')
    os.rename(r".\Models"+os.path.splitext(FileName)[0]+".sav", r".\Models\\"+aliceName+'.sav')
#    if KNNSelected:
#        os.rename(os.path.splitext(FileName)[0]+"_KNN.sav", aliceName+'.sav')
#    if DTSelected:
#        os.rename(os.path.splitext(FileName)[0]+"_DT.sav", aliceName+'.sav')
    return "Done"
    
    
    


#******************************************************************************
#predicting
#******************************************************************************


@app.route('/dsw/predict', methods=['POST'])   
def predict_result():
    print("Hiiii")
    modelName = request.args.get('modelName')
    print("Hiiii2")
    input_json_sample = request.get_json(force=True)  
    
    print("Hiii3")
    predictData=input_json_sample['rowData']
    print(predictData)
    classificationData=input_json_sample['Classification']
    precisionData=input_json_sample['Precision']
    saved_model = pickle.load(open(r".\Models\\"+modelName+".sav", 'rb'))
    prediction=saved_model.predict(predictData)
    tempJsonPredict=[]
    count=0
    for data in classificationData:
       tempJsonPredict.append({"classification":str(data),"LikelyResult":precisionData[count]*100})
       count += 1
        
    return json.dumps({"Result": str(prediction[0]),"Likely":tempJsonPredict})

###############################################################################
#Main
###############################################################################

if __name__ == '__main__':
    configuration = Configuration()
    # Configuring Proxies
    configuration.proxies()
    # Strating Server
    app.run('0.0.0.0',1612)
